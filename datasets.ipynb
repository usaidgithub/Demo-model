{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8113ea3-e35f-425f-8642-5a782cf01018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exploring 'METADATA' group:\n",
      "Subkeys in 'METADATA': ['AcquisitionInformation', 'DataQuality', 'DatasetIdentification', 'Extent', 'Lineage', 'ProcessStep', 'ProductSpecificationDocument', 'QADatasetIdentification', 'SeriesIdentification']\n",
      "\n",
      "Exploring 'ancillary_data' group:\n",
      "Subkeys in 'ancillary_data': ['atlas_sdp_gps_epoch', 'control', 'data_end_utc', 'data_start_utc', 'end_cycle', 'end_delta_time', 'end_geoseg', 'end_gpssow', 'end_gpsweek', 'end_orbit', 'end_region', 'end_rgt', 'granule_end_utc', 'granule_start_utc', 'release', 'start_cycle', 'start_delta_time', 'start_geoseg', 'start_gpssow', 'start_gpsweek', 'start_orbit', 'start_region', 'start_rgt', 'version', 'altimetry', 'atlas_engineering', 'calibrations', 'tep', 'gt2r', 'gt2l', 'gt3r', 'gt3l', 'gt1r', 'gt1l']\n",
      "\n",
      "Exploring 'atlas_impulse_response' group:\n",
      "Subkeys in 'atlas_impulse_response': ['pce1_spot1', 'pce2_spot3']\n",
      "\n",
      "Exploring 'ds_surf_type' group:\n",
      "Dataset 'ds_surf_type': (5,) int32\n",
      "\n",
      "Exploring 'ds_xyz' group:\n",
      "Dataset 'ds_xyz': (3,) int32\n",
      "\n",
      "Exploring 'gt1l' group:\n",
      "Subkeys in 'gt1l': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "\n",
      "Exploring 'gt1r' group:\n",
      "Subkeys in 'gt1r': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "\n",
      "Exploring 'gt2l' group:\n",
      "Subkeys in 'gt2l': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "\n",
      "Exploring 'gt2r' group:\n",
      "Subkeys in 'gt2r': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "\n",
      "Exploring 'gt3l' group:\n",
      "Subkeys in 'gt3l': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "\n",
      "Exploring 'gt3r' group:\n",
      "Subkeys in 'gt3r': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "\n",
      "Exploring 'orbit_info' group:\n",
      "Subkeys in 'orbit_info': ['bounding_polygon_lat1', 'bounding_polygon_lon1', 'crossing_time', 'cycle_number', 'lan', 'orbit_number', 'rgt', 'sc_orient', 'sc_orient_time']\n",
      "\n",
      "Exploring 'quality_assessment' group:\n",
      "Subkeys in 'quality_assessment': ['delta_time', 'qa_granule_fail_reason', 'qa_granule_pass_fail', 'gt1l', 'gt1r', 'gt2l', 'gt2r', 'gt3l', 'gt3r']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = 'ATL03_20240510223215_08142301_006_01.h5'  # Your file name\n",
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # Loop through all the keys and explore each one\n",
    "    for key in file.keys():\n",
    "        print(f\"\\nExploring '{key}' group:\")\n",
    "        if isinstance(file[key], h5py.Group):  # If it's a group, list its contents\n",
    "            print(f\"Subkeys in '{key}':\", list(file[key].keys()))\n",
    "        elif isinstance(file[key], h5py.Dataset):  # If it's a dataset, print its shape and type\n",
    "            dataset = file[key]\n",
    "            print(f\"Dataset '{key}':\", dataset.shape, dataset.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3ebfb92-8543-4123-8b21-549ac96e295e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subkeys in 'gt1l': ['bckgrd_atlas', 'geolocation', 'geophys_corr', 'heights', 'signal_find_output']\n",
      "Subkeys in 'gt1l/geolocation': ['altitude_sc', 'bounce_time_offset', 'delta_time', 'full_sat_fract', 'knn', 'near_sat_fract', 'neutat_delay_derivative', 'neutat_delay_total', 'neutat_ht', 'ph_index_beg', 'pitch', 'podppd_flag', 'range_bias_corr', 'ref_azimuth', 'ref_elev', 'reference_photon_index', 'reference_photon_lat', 'reference_photon_lon', 'roll', 'segment_dist_x', 'segment_id', 'segment_length', 'segment_ph_cnt', 'sigma_across', 'sigma_along', 'sigma_h', 'sigma_lat', 'sigma_lon', 'solar_azimuth', 'solar_elevation', 'surf_type', 'tx_pulse_energy', 'tx_pulse_skew_est', 'tx_pulse_width_lower', 'tx_pulse_width_upper', 'velocity_sc', 'yaw']\n",
      "Subkeys in 'gt1l/heights': ['delta_time', 'dist_ph_across', 'dist_ph_along', 'h_ph', 'lat_ph', 'lon_ph', 'pce_mframe_cnt', 'ph_id_channel', 'ph_id_count', 'ph_id_pulse', 'quality_ph', 'signal_conf_ph', 'weight_ph']\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "file_path = 'ATL03_20240510223215_08142301_006_01.h5'  # Your file path\n",
    "\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # Explore the 'gt1l' group\n",
    "    gt1l_keys = list(file['gt1l'].keys())\n",
    "    print(f\"Subkeys in 'gt1l': {gt1l_keys}\")\n",
    "    \n",
    "    # Explore 'geolocation' subkeys under 'gt1l'\n",
    "    if 'geolocation' in gt1l_keys:\n",
    "        geolocation_keys = list(file['gt1l/geolocation'].keys())\n",
    "        print(f\"Subkeys in 'gt1l/geolocation': {geolocation_keys}\")\n",
    "    \n",
    "    # Explore 'heights' subkeys under 'gt1l'\n",
    "    if 'heights' in gt1l_keys:\n",
    "        heights_keys = list(file['gt1l/heights'].keys())\n",
    "        print(f\"Subkeys in 'gt1l/heights': {heights_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa74dd7e-076d-470f-87f6-ca4d4998f8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Latitude   Longitude     Height    Delta_Time\n",
      "0 -0.002932 -122.664507 -15.840814  2.006155e+08\n",
      "1 -0.002913 -122.664509 -16.016417  2.006155e+08\n",
      "2 -0.002907 -122.664509 -19.940960  2.006155e+08\n",
      "3 -0.002907 -122.664509 -16.021324  2.006155e+08\n",
      "4 -0.002907 -122.664509 -16.066422  2.006155e+08\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'ATL03_20240510223215_08142301_006_01.h5'  # Your file path\n",
    "\n",
    "# Open the HDF5 file\n",
    "with h5py.File(file_path, 'r') as file:\n",
    "    # Extract important datasets like latitude, longitude, height, and time\n",
    "    latitudes = file['gt1l/heights/lat_ph'][:]\n",
    "    longitudes = file['gt1l/heights/lon_ph'][:]\n",
    "    heights = file['gt1l/heights/h_ph'][:]  # Example height data\n",
    "    delta_time = file['gt1l/heights/delta_time'][:]  # Time data\n",
    "\n",
    "# Create a pandas DataFrame with the extracted data\n",
    "data = {\n",
    "    'Latitude': latitudes,\n",
    "    'Longitude': longitudes,\n",
    "    'Height': heights,\n",
    "    'Delta_Time': delta_time\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "81f4a3e0-54df-444d-b055-757577e79938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1601266, 4)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b811aea3-387a-4146-85c8-a2baab506e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First ten rows of datasets    Latitude   Longitude     Height    Delta_Time\n",
      "0 -0.002932 -122.664507 -15.840814  2.006155e+08\n",
      "1 -0.002913 -122.664509 -16.016417  2.006155e+08\n",
      "2 -0.002907 -122.664509 -19.940960  2.006155e+08\n",
      "3 -0.002907 -122.664509 -16.021324  2.006155e+08\n",
      "4 -0.002907 -122.664509 -16.066422  2.006155e+08\n",
      "5 -0.002894 -122.664511 -15.945132  2.006155e+08\n",
      "6 -0.002862 -122.664514 -15.889396  2.006155e+08\n",
      "7 -0.002855 -122.664514 -15.701778  2.006155e+08\n",
      "8 -0.002842 -122.664516 -15.825165  2.006155e+08\n",
      "9 -0.002836 -122.664518   4.740792  2.006155e+08\n"
     ]
    }
   ],
   "source": [
    "print(\"First ten rows of datasets\",df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2027027-5d40-42de-9416-990aa0dc4c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random rows from datasets           Latitude   Longitude      Height    Delta_Time\n",
      "1324630  19.783059 -124.673339 -138.976105  2.006159e+08\n",
      "1214017  16.891456 -124.372575  356.000366  2.006158e+08\n",
      "468127    5.850877 -123.250547  -59.400513  2.006156e+08\n",
      "1241308  17.743370 -124.460777  333.974731  2.006158e+08\n",
      "1370806  21.030084 -124.804354   49.523533  2.006159e+08\n",
      "201488    2.044516 -122.869279   -3.786586  2.006156e+08\n",
      "524621    6.576791 -123.323500   12.005555  2.006156e+08\n",
      "312274    3.135426 -122.978425    2.185738  2.006156e+08\n",
      "158512    1.574013 -122.822207   -8.637660  2.006156e+08\n",
      "1002362  13.928182 -124.067930   59.084339  2.006158e+08\n"
     ]
    }
   ],
   "source": [
    "print(\"Random rows from datasets\",df.sample(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f3f4c9b-3cb0-4fed-aa10-d5ef5680b15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('glacial_lake_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e636434-4808-452c-ba0c-89db0a1157f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latitude      0\n",
      "Longitude     0\n",
      "Height        0\n",
      "Delta_Time    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# If there are missing values, you can drop them or fill them with a strategy\n",
    "df = df.dropna()  # Option to drop rows with missing values\n",
    "\n",
    "# Feature Scaling (Normalizing Latitude, Longitude, Height, and Delta_Time)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df[['Latitude', 'Longitude', 'Height', 'Delta_Time']] = scaler.fit_transform(df[['Latitude', 'Longitude', 'Height', 'Delta_Time']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92eb3093-8875-4bb0-99c7-ef683c39044a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOF_Risk\n",
      "0    1439803\n",
      "1     161463\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inverse transform the entire feature set that was scaled (Latitude, Longitude, Height, Delta_Time)\n",
    "original_values = scaler.inverse_transform(df[['Latitude', 'Longitude', 'Height', 'Delta_Time']])\n",
    "\n",
    "# Extract the original Height values (column index 2, as it's the third column)\n",
    "original_height = original_values[:, 2]\n",
    "\n",
    "\n",
    "# Apply the GLOF risk threshold based on the original height\n",
    "certain_threshold = -20  # Set based on domain knowledge\n",
    "df['GLOF_Risk'] = np.where(original_height < certain_threshold, 1, 0)\n",
    "\n",
    "# Check the distribution of GLOF risk\n",
    "print(df['GLOF_Risk'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121d22ee-2b9b-4139-b28a-2fa217b46cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Latitude', 'Longitude', 'Delta_Time']]  # Feature set\n",
    "y = df['GLOF_Risk']  # Target: 1 for GLOF, 0 for safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f78a056-d55a-4bd3-9438-82e9941882c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (1281012, 3), Test set: (320254, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Define X (features) and y (target)\n",
    "X = df[['Latitude', 'Longitude', 'Delta_Time']]  # Features\n",
    "y = df['GLOF_Risk']  # Target (Binary classification)\n",
    "\n",
    "# Split the dataset into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92dcc602-0157-451b-8b95-28c710a0febb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy after SMOTE:\n",
      " 0.9303551522602019\n",
      "Confusion Matrix after SMOTE:\n",
      " [[266055  22196]\n",
      " [ 17914 269757]]\n",
      "\n",
      "Classification Report after SMOTE:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93    288251\n",
      "           1       0.92      0.94      0.93    287671\n",
      "\n",
      "    accuracy                           0.93    575922\n",
      "   macro avg       0.93      0.93      0.93    575922\n",
      "weighted avg       0.93      0.93      0.93    575922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_smote, y_smote = smote.fit_resample(X, y)\n",
    "\n",
    "# Split the balanced dataset into training and testing sets\n",
    "X_train_smote, X_test_smote, y_train_smote, y_test_smote = train_test_split(X_smote, y_smote, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Random Forest Classifier on the balanced dataset\n",
    "clf_smote = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf_smote.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_smote = clf_smote.predict(X_test_smote)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_smote = accuracy_score(y_test_smote, y_pred_smote)\n",
    "print(\"Accuracy after SMOTE:\\n\", accuracy_smote)\n",
    "print(\"Confusion Matrix after SMOTE:\\n\", confusion_matrix(y_test_smote, y_pred_smote))\n",
    "print(\"\\nClassification Report after SMOTE:\\n\", classification_report(y_test_smote, y_pred_smote))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b560a7e7-1726-4e5f-bf83-b3fc4e0be72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This location is in a safe zone.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Example user inputs (these would come from your app or live location)\n",
    "user_latitude = 33.2778  # Example Latitude\n",
    "user_longitude = 75.3412   # Example Longitude\n",
    "user_time = datetime.now().timestamp()  # Current time as Delta_Time\n",
    "user_height = 0  # Height is included but won't be used in the model\n",
    "\n",
    "# Step 1: Create a DataFrame with user input including Height\n",
    "input_data = pd.DataFrame({\n",
    "    'Latitude': [user_latitude],\n",
    "    'Longitude': [user_longitude],\n",
    "    'Height': [user_height],\n",
    "    'Delta_Time': [user_time] # Include height for other purposes if necessary\n",
    "})\n",
    "\n",
    "# Step 2: Standardize the entire input data (including 'Height')\n",
    "input_data_scaled = scaler.transform(input_data)\n",
    "\n",
    "# Convert the scaled NumPy array back to a DataFrame for easier column handling\n",
    "input_data_scaled_df = pd.DataFrame(input_data_scaled, columns=input_data.columns)\n",
    "\n",
    "# Step 3: Remove the 'Height' column from the scaled data\n",
    "input_data_for_model = input_data_scaled_df.drop(columns=['Height'])\n",
    "\n",
    "# Step 4: Predict whether the input location is at risk of GLOF using the trained model\n",
    "risk_prediction = clf_smote.predict(input_data_for_model)\n",
    "\n",
    "# Step 5: Output the result based on the prediction\n",
    "if risk_prediction == 1:\n",
    "    print(\"This location is in a high-risk GLOF zone.\")\n",
    "else:\n",
    "    print(\"This location is in a safe zone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c88ba22-4d93-493c-a77d-036073e37cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
